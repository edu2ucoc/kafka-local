# -------------
# 기본 구성, 로그 레벨, 파서 파일 설정등
# 키값 첫글자 대문자, 단어가 이어지면 _(스네이크), 새로운 단어 첫글자 대문자
# 스네이크 + 카멜 표기법 
# -------------
[SERVICE]
    Flush           1
    Log_Level       info
    Parsers_File    parsers.conf

# -------------
# kafka Cusumer
# -------------
[INPUT]
    Name            kafka
    # 카프카 서비스의 내부 접속 주소
    Brokers         kafka:29092     
    # 프로듀서가 공급하는 메세지 토픽
    Topics          web_logs        
    Group_Id        fluent-bit-group
    # 메세지 포멧
    Format          json
    # 태그 (원본 데이터의 표식)
    Tag             raw.logs        

# -------------
# 데이터 전처리 : 구조 변경, 필요시 특정 데이터 삭제 등등.. 
# -------------
# payload 데이터를 추출하여 payload의 같은 레벨로 배치 => 평탄화
[FILTER]
    Name              nest
    # raw.logs 태그가 붙어있는 데이터를 대상(매칭)
    Match             raw.logs
    # lift -> 꺼낸다 (상위 레벨로 이동)
    Operation         lift
    # 대상 키값
    Nested_under      payload

# 클론, raw.logs 태그만 있는 데이터 대상으로 
# 조건 : ip라는 키값이 존재하는 데이터를 대상으로 -> processed.logs 태그를 추가하여 복제
[FILTER]
    Name              rewrite_tag
    Match             raw.logs
    # raw.logs라는 테그가 존재하는 데이터들 중에서 
    # ip라는 필드가 존재하고, .*:데이터가 어떤것이든 존재만하면, 
    # processed.logs 태그 추가하여, true:복제
    Rule              ip .* processed.logs true
    Emitter_Name      re_emitted_logs


# 오픈서치 전용 태그(복사본에 작업), 특정 데이터파트 삭제(전처리간주)
# 사본조작 => 전처리, 오픈서치용(가동된 데이터만 업로그), s3(원데이터 업로드)
[FILTER]
    Name              preprocessing
    Match             processed.logs
    # 불필요한 필드 제거
    Remove_key        topic
    Remove_key        partition
    Remove_key        offset
    Remove_key        error
    Remove_key        key
    # 원데이터에 있던 필드
    Remove_key        user_agent
    # 없는 필드 작성 테스트(데이터별로 특정 필드가 있을수 없을수도 있으므로)
    Remove_key        evt


# -------------
# S3 업로드(raw data)
# -------------
# [OUTPUT]
# -------------
# opensearch 업로드(preprocessing data)
# -------------
# [OUTPUT]
# -------------
# 로깅 (개발시 데이터 형태 확인)
# -------------
[OUTPUT]
    Name        stdout
    # 모든 메세지 출력
    Match       *