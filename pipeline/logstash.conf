# 입력
input {
    # 카프카
    kafka {
        bootstrap_servers => "kafka:29092"  # 내부용 통신 프로토콜:포트
        topics => ["web_logs"]              # 토픽, n개 될수 있음
        codec => "json"                     # 데이터(메세지)의 형식
        auto_offset_reset => "latest"       # 최신 데이터부터 읽기
        group_id => "logstash-aws-group-v1" # 처음부터 다시 메세지를 읽어야 한다면. 필요 조건
    }
}

# logstash의 표준 문서에서 표기법 확인
# 필터 -> 원본(s3용) -> 사본 구성 -> 전처리 -> opensearch용 표현
filter {
  # 1. 들어오는 모든 데이터에 태그를 추가 ("raw_s3") -> 원본은 이대로 마무리
  mutate {
    add_tag => ["raw_s3"]
  }
  # 2. 데이터 복제 -> "processed_copy" 라는 태그추가
  clone {
    clones => ["processed_copy"]
  }
  # 3. 태그중 processed_copy 가 있으면, 원본은 두고, 사본만 조작
  if "processed_copy" in [tags] {    
    mutate {
      # "raw_s3", "processed_copy" 태그 제거
      remove_tag => ["raw_s3"]
      remove_tag => ["processed_copy"]
      # "for_opensearch" 태그 추가
      add_tag => ["for_opensearch"]
      # 타입 필드 제거
      remove_field => ["type"]
    }
    # 원본은 태그로 오직 "raw_s3"만 가짐
    # 사본은 태그로 오직 "for_opensearch" 만 가짐
    
    # user-agent 필드값 파싱
    useragent {
      source => "user_agent"
      target => "ua_parsed"
    }

    # 간단한 전처리 => 불필요한 필드 제거 => 용량 줄임 => 비용 절감
    mutate {
      remove_field => ["user_agent", "[event][original]"]
    }
  }
}



# 출력, 조건을 달아서 각각의 출력 방향으로 데이터를 전송
# 데이터를 가공하여 특정 방향으로 데이터를 송출할수 있음
output {
    # 원본 데이터
    # 어떤 데이터든 간에 태그값에 "raw_s3" 있으면 s3로 전송
    if "raw_s3" in [tags] {
        s3 {
            region => "ap-northeast-2"
            bucket => "${AWS_S3_BUCKET}"
            prefix => "raw_logs/%{+YYYY}/%{+MM}/%{+dd}/" # 저장될 경로
            # 엑세스키
            access_key_id => "${AWS_ACCESS_KEY_ID}"
            secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
            # 데이터 전송 시간, 크기, 형식
            # 현제 데이터는 303 Bytes 크기임
            size_file => 102400  # 100KB -> 버퍼 크기를 작게 구성 -> 빠르게 업로드(단, 데이터가 크면 쪼개짐->더느려짐)
            time_file => 1       # 1분
            codec => "json_lines"
        }
    }
    # 가공(전처리된)된 데이터 (간단한 전처리)
    if "for_opensearch" in [tags] {
        opensearch {
            # aws 엔드 포인트
            hosts => ["https://${AWS_OPENSEARCH_HOST}:443"]
            # 인증 정보 필요
            user => "${AWS_OPENSEARCH_USER}"
            password => "${AWS_OPENSEARCH_PASS}"
            # 인덱스 이름 부여 -> 인덱스명 단위가 일단위
            index => "web-logs-realtime-%{+YYYY.MM.dd}"
            # 보안정보
            ssl => true
            ssl_certificate_verification => true
            # 기타설정
            manage_template => false
        }
    }
}