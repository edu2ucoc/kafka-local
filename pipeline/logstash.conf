# 입력
input {
    # 카프카
    kafka {
        bootstrap_servers => "kafka:29092"  # 내부용 통신 프로토콜:포트
        topics => ["web_logs"]              # 토픽, n개 될수 있음
        codec => "json"                     # 데이터(메세지)의 형식
        auto_offset_reset => "latest"       # 최신 데이터부터 읽기
        group_id => "logstash-aws-group-v1" # 처음부터 다시 메세지를 읽어야 한다면. 필요 조건
    }
}

# 필터
filter {
  mutate {
    add_tag => ["raw_s3"]
  }
  clone {
    clones => ["processed_copy"]
  }
  if [type] == "processed_copy" {
    mutate {
      remove_tag => ["raw_s3"]
      add_tag => ["for_opensearch"]
      remove_field => ["type"]
    }
    useragent {
      source => "user_agent"
      target => "ua_parsed"
    }
    mutate {
      remove_field => ["user_agent", "[event][original]"]
    }
  }
}



# 출력, 조건을 달아서 각각의 출력 방향으로 데이터를 전송
# 데이터를 가공하여 특정 방향으로 데이터를 송출할수 있음
output {
    # 원본 데이터
    s3 {
        region => "ap-northeast-2"
        bucket => "${AWS_S3_BUCKET}"
        prefix => "raw_logs/%{+YYYY}/%{+MM}/%{+dd}/" # 저장될 경로
        # 엑세스키
        access_key_id => "${AWS_ACCESS_KEY_ID}"
        secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
    }
    # 가공(전처리된)된 데이터 (간단한 전처리)
    opensearch {
        # aws 엔드 포인트
        hosts => ["https://${AWS_OPENSEARCH_HOST}:443"]
        # 인증 정보 필요
        user => "${AWS_OPENSEARCH_USER}"
        password => "${AWS_OPENSEARCH_PASS}"
        # 인덱스 이름 부여 -> 인덱스명 단위가 일단위
        index => "web-logs-realtime-%{+YYYY.MM.dd}"
        # 보안정보
        # 기타설정
    }    
}